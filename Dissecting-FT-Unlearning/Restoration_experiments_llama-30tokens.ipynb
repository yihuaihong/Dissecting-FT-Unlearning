{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Find {gpu_count} GPU can be used.\")\n",
    "\n",
    "    # 遍历并打印每个GPU设备的名称\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i + 1}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU can be used.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "import copy\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "# from bert_score import score\n",
    "import statistics\n",
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random.seed(8888)\n",
    "torch.manual_seed(8888)\n",
    "random.seed(8888)\n",
    "np.random.seed(8888)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(8888)\n",
    "    torch.cuda.manual_seed_all(8888)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "torch.cuda.set_device(0)\n",
    "\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "# Get CounterFact data for GPT2-xl, from the ROME repository.\n",
    "#wget.download(\"https://rome.baulab.info/data/dsets/known_1000.json\")\n",
    "\n",
    "\n",
    "model_dir = \"/mnt/workspace/workgroup/yhhong/transformers\"\n",
    "original_model_name = \"Llama-2-7b-chat-hf\" #\"chatglm3-6b\" #\"Llama-2-7b-chat-hf\" #\"Qwen1.5-7B-Chat\"\n",
    "\n",
    "\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    join(model_dir, original_model_name),\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True\n",
    ");\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(join(model_dir, original_model_name))\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "original_model.to('cuda');\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "original_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def set_act_get_hooks(model, mlp_coef=False, attn=False):\n",
    "    \"\"\"\n",
    "    Works on LLaMA-2, getting coef values or attn values\n",
    "    \"\"\"\n",
    "\n",
    "    for attr in [\"activations_\"]:\n",
    "        if not hasattr(model, attr):\n",
    "            setattr(model, attr, {})\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(module, input, output):\n",
    "            if \"m_coef\" in name:\n",
    "                model.activations_[name] = input[0][:, :].detach() \n",
    "            elif \"attn\" in name:\n",
    "                model.activations_[name] = output[:, :].detach() \n",
    "\n",
    "        return hook\n",
    "\n",
    "    hooks = []\n",
    "    for i in range(32):\n",
    "        if mlp_coef is True: #co-effciency\n",
    "            hooks.append(model.model.layers[i].mlp.down_proj.register_forward_hook(get_activation(\"m_coef_\" + str(i))))\n",
    "        if attn is True:\n",
    "            hooks.append(model.model.layers[i].self_attn.o_proj.register_forward_hook(get_activation(\"attn_\" + str(i))))    \n",
    "        \n",
    "\n",
    "    return hooks\n",
    "\n",
    "def set_act_modify_hooks(model, original_model, mlp_coef, attn, ori_mlp_coef, ori_attn, un_mlp_coef, un_attn, layers=None):\n",
    "    \"\"\"\n",
    "    Works on LLaMA-2, modifying coef values or attn values\n",
    "    \"\"\"\n",
    "\n",
    "    def modify_activation(name, update_value, patch_input):\n",
    "        def pre_hook(module, input):\n",
    "            if \"m_coef\" in name:\n",
    "                input[0][:, :, :] = update_value \n",
    "        \n",
    "        def post_hook(module, input, output):\n",
    "            if \"attn\" in name:\n",
    "                output[:, :, :] = update_value\n",
    "                \n",
    "        if patch_input:\n",
    "            return pre_hook\n",
    "        else:\n",
    "            return post_hook\n",
    "\n",
    "    hooks = []\n",
    "    if layers is not None:\n",
    "        for i in range(32):\n",
    "            if i in layers and mlp_coef is True: #co-effciency\n",
    "                hooks.append(model.model.layers[i].mlp.down_proj.register_forward_pre_hook(modify_activation(\"m_coef_\" + str(i), update_value = ori_mlp_coef[i], patch_input=True)))\n",
    "            # else:\n",
    "            #     hooks.append(model.model.layers[i].mlp.down_proj.register_forward_pre_hook(modify_activation(\"m_coef_\" + str(i), update_value = un_mlp_coef[i], patch_input=True)))\n",
    "            if i in layers and attn is True:\n",
    "                hooks.append(model.model.layers[i].self_attn.o_proj.register_forward_hook(modify_activation(\"attn_\" + str(i), update_value = ori_attn[i], patch_input=False)))  \n",
    "            # else:\n",
    "            #     hooks.append(model.model.layers[i].self_attn.o_proj.register_forward_hook(modify_activation(\"attn_\" + str(i), update_value = un_attn[i], patch_input=False)))  \n",
    "\n",
    "    return hooks\n",
    "\n",
    "def remove_hooks(hooks):\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "        \n",
    "def compute_loss(target_logits, logits):\n",
    "    \n",
    "    #MSE loss\n",
    "\n",
    "    return torch.mean((target_logits - logits) ** 2)\n",
    "\n",
    "\n",
    "def compute_KRS(ori_loss, new_loss):  \n",
    "    return 1 - (new_loss / ori_loss)\n",
    "    \n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "tags": []
   },
   "cell_type": "code",
   "source": [
    "# loading data:\n",
    "\n",
    "with open(\"data/llama2-7b_concepts_test_30tokens_answers.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "for ix, item in enumerate(data):\n",
    "    print(ix,\" \",item['Concept'])\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "# evaluate original model and obtain the next 30 tokens answers\n",
    "\n",
    "for index, x in enumerate(tqdm(data)):\n",
    "    \n",
    "    questions = []\n",
    "    n_new_tokens = 30\n",
    "    for idx, question in enumerate(x['QA']):\n",
    "        questions.append(f\"Question: {question}\\n Answer:\")\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to('cuda')\n",
    "    input_length = inputs.input_ids.size(1)\n",
    "    with torch.no_grad():\n",
    "        generation_output = original_model.generate(  # mt.model\n",
    "            **inputs,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=n_new_tokens,\n",
    "        )\n",
    "\n",
    "    \n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "    normal_tokens_num_list = []\n",
    "    for i, output in enumerate(generation_output):\n",
    "        eos_count = (output == eos_token_id).sum().item()\n",
    "        total_tokens = output.size(0) \n",
    "        normal_tokens_count = total_tokens - eos_count - inputs.input_ids.size(1)\n",
    "        normal_tokens_num_list.append(normal_tokens_count)\n",
    "    \n",
    "    outputs = tokenizer.batch_decode(generation_output[:, :], skip_special_tokens=True)\n",
    "    \n",
    "    data[index]['QA_with_answers'] = outputs\n",
    "    data[index]['answers_tokens_num'] = normal_tokens_num_list\n",
    "\n",
    "with open(\"data/llama2-7b_concepts_test_30tokens_answers.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "with open(\"data/llama2-7b_concepts_test_30tokens_answers.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "#on vanilla model and unlearned model\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "model_dir = \"/mnt/workspace/workgroup/yhhong/unlearn_results\"\n",
    "\n",
    "methods_ori_loss_list = [] \n",
    "methods_ori_attn_list = []\n",
    "methods_ori_mlp_coef_list = []\n",
    "methods_un_mlp_coef_list = []\n",
    "methods_un_attn_list = []\n",
    "\n",
    "for method in ['grad_diff', 'dpo']:\n",
    "    ori_loss_list = [] \n",
    "    ori_attn_list = []\n",
    "    ori_mlp_coef_list = []\n",
    "    un_mlp_coef_list = []\n",
    "    un_attn_list = []\n",
    "    \n",
    "    for x in tqdm(data):\n",
    "        # Loading new unlearned model for certain concept \n",
    "\n",
    "        model_name = f\"llama2-7b/{method}/Full/{x['id']}\" \n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            join(model_dir, model_name),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        );\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(join(model_dir, model_name))\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        model.to('cuda')\n",
    "        \n",
    "        questions = x['QA_with_answers']\n",
    "\n",
    "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to('cuda')\n",
    "        input_length = inputs.attention_mask.sum(dim=1)\n",
    "\n",
    "        un_hooks = set_act_get_hooks(model, mlp_coef=True, attn=True)\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs)\n",
    "\n",
    "        # remove hooks\n",
    "        remove_hooks(un_hooks) \n",
    "\n",
    "        hooks = set_act_get_hooks(original_model, mlp_coef=True, attn=True)\n",
    "        with torch.no_grad():\n",
    "            original_output = original_model(**inputs)\n",
    "        # remove hooks\n",
    "        remove_hooks(hooks) \n",
    "        \n",
    "        \n",
    "        loss_list = []\n",
    "        for i, length in enumerate(x['answers_tokens_num']):\n",
    "            loss = compute_loss(original_output.logits[i, -length:, :], output.logits[i, -length:, :])\n",
    "            loss_list.append(loss)\n",
    "        \n",
    "        un_mlp_coef = []\n",
    "        un_attn = []\n",
    "        ori_mlp_coef = []\n",
    "        ori_attn = []\n",
    "        for layer in range(32):\n",
    "            un_mlp_coef.append(model.activations_[f'm_coef_{layer}'])\n",
    "            un_attn.append(model.activations_[f'attn_{layer}'])\n",
    "            ori_mlp_coef.append(original_model.activations_[f'm_coef_{layer}'])\n",
    "            ori_attn.append(original_model.activations_[f'attn_{layer}'])\n",
    "\n",
    "\n",
    "        ori_loss_list.append(loss_list)\n",
    "\n",
    "        un_mlp_coef_list.append(un_mlp_coef) # on all data\n",
    "        un_attn_list.append(un_attn) # on all data\n",
    "\n",
    "        ori_mlp_coef_list.append(ori_mlp_coef) # on all data\n",
    "        ori_attn_list.append(ori_attn) # on all data\n",
    "\n",
    "\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    methods_ori_loss_list.append(ori_loss_list)\n",
    "    methods_ori_attn_list.append(ori_attn_list)\n",
    "    methods_ori_mlp_coef_list.append(ori_mlp_coef_list)\n",
    "    methods_un_mlp_coef_list.append(un_mlp_coef_list)\n",
    "    methods_un_attn_list.append(un_attn_list)\n",
    "    \n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "#patching on MLP coefficients on unlearned models\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "model_dir = \"/mnt/workspace/workgroup/yhhong/unlearn_results\"\n",
    "\n",
    "\n",
    "methods_mlp_coef_patch_loss_lists = []\n",
    "for j, method in enumerate(['grad_diff', 'dpo']):\n",
    "    mlp_coef_patch_loss_lists = []\n",
    "    ori_mlp_coef_list = methods_ori_mlp_coef_list[j]\n",
    "    ori_attn_list = methods_ori_attn_list[j]\n",
    "    un_mlp_coef_list = methods_un_mlp_coef_list[j]\n",
    "    un_attn_list = methods_un_attn_list[j]\n",
    "    \n",
    "    for index, x in enumerate(tqdm(data)):\n",
    "\n",
    "        # Loading new unlearned model for certain concept \n",
    "\n",
    "        model_name = f\"llama2-7b/{method}/Full/{x['id']}\" #\"chatglm3-6b\" #\"Llama-2-7b-chat-hf\" #\"Qwen1.5-7B-Chat\"\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            join(model_dir, model_name),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        );\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(join(model_dir, model_name))\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        model.to('cuda');\n",
    "\n",
    "        questions = x['QA_with_answers']\n",
    "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to('cuda')\n",
    "        input_length = inputs.attention_mask.sum(dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            original_output = original_model(**inputs)\n",
    "\n",
    "        patch_loss_list = []\n",
    "        for layer in range(32):\n",
    "            hooks = set_act_modify_hooks(model, original_model, mlp_coef=True, attn=False, ori_mlp_coef=ori_mlp_coef_list[index], ori_attn=ori_attn_list[index], un_mlp_coef=un_mlp_coef_list[index], un_attn=un_attn_list[index], layers = [layer-2, layer-1, layer, layer+1, layer+2])\n",
    "            with torch.no_grad():\n",
    "                output = model(**inputs)\n",
    "            # remove hooks\n",
    "            remove_hooks(hooks) \n",
    "            \n",
    "            loss_list = []\n",
    "            for i, length in enumerate(x['answers_tokens_num']):\n",
    "                loss = compute_loss(original_output.logits[i, -length:, :], output.logits[i, -length:, :])\n",
    "                loss_list.append(loss)\n",
    "            \n",
    "            patch_loss_list.append(loss_list)\n",
    "\n",
    "        mlp_coef_patch_loss_lists.append(patch_loss_list)   \n",
    "\n",
    "\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    methods_mlp_coef_patch_loss_lists.append(mlp_coef_patch_loss_lists)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "print('recover on LLaMA by patching on coef (5 layer per patch)')\n",
    "        \n",
    "coef_KRS_list_all = []\n",
    "for t, method in enumerate(['grad_diff', 'dpo']):\n",
    "    print(f'on {method}')\n",
    "    coef_KRS_list_per_method = []\n",
    "    for patch_losses, ori_losses in zip(methods_mlp_coef_patch_loss_lists[t], methods_ori_loss_list[t]):\n",
    "        coef_KRS_list_per_data = []\n",
    "        for new_losses in patch_losses: #different_layer\n",
    "            coef_KRS_list_per_layer = []\n",
    "            for new_loss, ori_loss in zip(new_losses, ori_losses):\n",
    "                KRS = compute_KRS(ori_loss, new_loss=new_loss)\n",
    "                coef_KRS_list_per_layer.append(KRS.cpu())\n",
    "    \n",
    "            coef_KRS_list_per_data.append(np.mean(coef_KRS_list_per_layer, axis=-1))\n",
    "        coef_KRS_list_per_method.append(coef_KRS_list_per_data)\n",
    "     \n",
    "    coef_KRS_list_per_method = np.array(coef_KRS_list_per_method)\n",
    "    avg_coef_KRS_list_per_method = np.mean(coef_KRS_list_per_method, axis=0)\n",
    "    \n",
    "    coef_KRS_list_all.append(avg_coef_KRS_list_per_method)\n",
    "\n",
    "coef_KRS_list_all = np.array(coef_KRS_list_all)\n",
    "avg_coef_KRS_list_all = np.mean(coef_KRS_list_all, axis=0)\n",
    "print(f'avg_coef_KRS_list_all: {list(avg_coef_KRS_list_all)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#patching on Attention states on unlearned models\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "model_dir = \"/mnt/workspace/workgroup/yhhong/unlearn_results\"\n",
    "\n",
    "methods_attn_patch_loss_lists = []\n",
    "for j, method in enumerate(['grad_diff', 'dpo']):\n",
    "    attn_patch_loss_lists = []\n",
    "    ori_mlp_coef_list = methods_ori_mlp_coef_list[j]\n",
    "    ori_attn_list = methods_ori_attn_list[j]\n",
    "    un_mlp_coef_list = methods_un_mlp_coef_list[j]\n",
    "    un_attn_list = methods_un_attn_list[j]\n",
    "    \n",
    "    for index, x in enumerate(tqdm(data)):\n",
    "\n",
    "        # Loading new unlearned model for certain concept \n",
    "\n",
    "        model_name = f\"llama2-7b/{method}/Full/{x['id']}\" \n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            join(model_dir, model_name),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        );\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(join(model_dir, model_name))\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        model.to('cuda');\n",
    "\n",
    "        questions = x['QA_with_answers']\n",
    "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to('cuda')\n",
    "        input_length = inputs.attention_mask.sum(dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            original_output = original_model(**inputs)\n",
    "\n",
    "        patch_loss_list = []\n",
    "        for layer in range(32):\n",
    "            hooks = set_act_modify_hooks(model, original_model, mlp_coef=False, attn=True, ori_mlp_coef=ori_mlp_coef_list[index], ori_attn=ori_attn_list[index], un_mlp_coef=un_mlp_coef_list[index], un_attn=un_attn_list[index], layers = [layer-2, layer-1, layer, layer+1, layer+2]) #[i for i in range(32)]\n",
    "            with torch.no_grad():\n",
    "                output = model(**inputs)\n",
    "            # remove hooks\n",
    "            remove_hooks(hooks) \n",
    "            \n",
    "            loss_list = []\n",
    "            for i, length in enumerate(x['answers_tokens_num']):\n",
    "                loss = compute_loss(original_output.logits[i, -length:, :], output.logits[i, -length:, :])\n",
    "                loss_list.append(loss)\n",
    "            \n",
    "            patch_loss_list.append(loss_list)\n",
    "\n",
    "        attn_patch_loss_lists.append(patch_loss_list)   \n",
    "\n",
    "\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    methods_attn_patch_loss_lists.append(attn_patch_loss_lists)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "print('recover on LLaMA by patching on attn (5 layer per patch)')\n",
    "        \n",
    "attn_KRS_list_all = []\n",
    "for t, method in enumerate(['grad_diff', 'dpo']):\n",
    "    print(f'on {method}')\n",
    "    attn_KRS_list_per_method = []\n",
    "    for patch_losses, ori_losses in zip(methods_attn_patch_loss_lists[t], methods_ori_loss_list[t]):\n",
    "        attn_KRS_list_per_data = []\n",
    "        for new_losses in patch_losses: #different_layer\n",
    "            attn_KRS_list_per_layer = []\n",
    "            for new_loss, ori_loss in zip(new_losses, ori_losses):\n",
    "                KRS = compute_KRS(ori_loss, new_loss=new_loss)\n",
    "                attn_KRS_list_per_layer.append(KRS.cpu())\n",
    "    \n",
    "            attn_KRS_list_per_data.append(np.mean(attn_KRS_list_per_layer, axis=-1))\n",
    "        attn_KRS_list_per_method.append(attn_KRS_list_per_data)\n",
    "     \n",
    "    attn_KRS_list_per_method = np.array(attn_KRS_list_per_method)\n",
    "    avg_attn_KRS_list_per_method = np.mean(attn_KRS_list_per_method, axis=0)\n",
    "    \n",
    "    attn_KRS_list_all.append(avg_attn_KRS_list_per_method)\n",
    "\n",
    "attn_KRS_list_all = np.array(attn_KRS_list_all)\n",
    "avg_attn_KRS_list_all = np.mean(attn_KRS_list_all, axis=0)\n",
    "print(f'avg_attn_KRS_list_all: {list(avg_attn_KRS_list_all)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#patching on both MLP coef and Attention states on unlearned models\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "\n",
    "model_dir = \"/mnt/workspace/workgroup/yhhong/unlearn_results\"\n",
    "\n",
    "methods_attn_mlp_coef_patch_loss_lists = []\n",
    "for j, method in enumerate(['grad_diff', 'dpo']):\n",
    "    attn_mlp_coef_patch_loss_lists = []\n",
    "    ori_mlp_coef_list = methods_ori_mlp_coef_list[j]\n",
    "    ori_attn_list = methods_ori_attn_list[j]\n",
    "    un_mlp_coef_list = methods_un_mlp_coef_list[j]\n",
    "    un_attn_list = methods_un_attn_list[j]\n",
    "\n",
    "    for index, x in enumerate(tqdm(data)):\n",
    "\n",
    "        model_name = f\"llama2-7b/{method}/Full/{x['id']}\" \n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            join(model_dir, model_name),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(join(model_dir, model_name))\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        model.to('cuda');\n",
    "\n",
    "        questions = x['QA_with_answers']\n",
    "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to('cuda')\n",
    "        input_length = inputs.attention_mask.sum(dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            original_output = original_model(**inputs)\n",
    "\n",
    "        patch_loss_list = []\n",
    "        for layer in range(32):\n",
    "            hooks = set_act_modify_hooks(model, original_model, mlp_coef=True, attn=True, ori_mlp_coef=ori_mlp_coef_list[index], ori_attn=ori_attn_list[index], un_mlp_coef=un_mlp_coef_list[index], un_attn=un_attn_list[index], layers = [layer-2, layer-1, layer, layer+1, layer+2])\n",
    "            with torch.no_grad():\n",
    "                output = model(**inputs)\n",
    "            # remove hooks\n",
    "            remove_hooks(hooks) \n",
    "            \n",
    "            loss_list = []\n",
    "            for i, length in enumerate(x['answers_tokens_num']):\n",
    "                loss = compute_loss(original_output.logits[i, -length:, :], output.logits[i, -length:, :])\n",
    "                loss_list.append(loss)\n",
    "            \n",
    "            patch_loss_list.append(loss_list)\n",
    "\n",
    "        attn_mlp_coef_patch_loss_lists.append(patch_loss_list)   \n",
    "\n",
    "\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    methods_attn_mlp_coef_patch_loss_lists.append(attn_mlp_coef_patch_loss_lists)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "print('recover on LLaMA by patching on both coef and attn (5 layer per patch)')\n",
    "        \n",
    "attn_mlp_coef_KRS_list_all = []\n",
    "for t, method in enumerate(['grad_diff', 'dpo']):\n",
    "    print(f'on {method}')\n",
    "    attn_mlp_coef_KRS_list_per_method = []\n",
    "    for patch_losses, ori_losses in zip(methods_attn_mlp_coef_patch_loss_lists[t], methods_ori_loss_list[t]):\n",
    "        attn_mlp_coef_KRS_list_per_data = []\n",
    "        for new_losses in patch_losses: #different_layer\n",
    "            attn_mlp_coef_KRS_list_per_layer = []\n",
    "            for new_loss, ori_loss in zip(new_losses, ori_losses):\n",
    "                KRS = compute_KRS(ori_loss, new_loss=new_loss)\n",
    "                attn_mlp_coef_KRS_list_per_layer.append(KRS.cpu())\n",
    "    \n",
    "            attn_mlp_coef_KRS_list_per_data.append(np.mean(attn_mlp_coef_KRS_list_per_layer, axis=-1))\n",
    "        attn_mlp_coef_KRS_list_per_method.append(attn_mlp_coef_KRS_list_per_data)\n",
    "     \n",
    "    attn_mlp_coef_KRS_list_per_method = np.array(attn_mlp_coef_KRS_list_per_method)\n",
    "    avg_attn_mlp_coef_KRS_list_per_method = np.mean(attn_mlp_coef_KRS_list_per_method, axis=0)\n",
    "    \n",
    "    attn_mlp_coef_KRS_list_all.append(avg_attn_mlp_coef_KRS_list_per_method)\n",
    "\n",
    "attn_mlp_coef_KRS_list_all = np.array(attn_mlp_coef_KRS_list_all)\n",
    "avg_attn_mlp_coef_KRS_list_all = np.mean(attn_mlp_coef_KRS_list_all, axis=0)\n",
    "print(f'avg_attn_mlp_coef_KRS_list_all: {list(avg_attn_mlp_coef_KRS_list_all)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "#Restore MLP value vectors on unlearned models\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from os.path import join\n",
    "import copy\n",
    "\n",
    "global old_params\n",
    "\n",
    "\n",
    "model_dir = \"/mnt/workspace/workgroup/yhhong/unlearn_results\"\n",
    "\n",
    "methods_vectors_patch_loss_lists = []\n",
    "for j, method in enumerate(['grad_diff', 'dpo']):\n",
    "    vectors_patch_loss_lists = []\n",
    "    ori_mlp_coef_list = methods_ori_mlp_coef_list[j]\n",
    "    ori_attn_list = methods_ori_attn_list[j]\n",
    "    un_mlp_coef_list = methods_un_mlp_coef_list[j]\n",
    "    un_attn_list = methods_un_attn_list[j]\n",
    "\n",
    "    for index, x in enumerate(tqdm(data)):\n",
    "\n",
    "        model_name = f\"llama2-7b/{method}/Full/{x['id']}\" \n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            join(model_dir, model_name),\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        old_params = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(join(model_dir, model_name))\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        model.to('cuda')\n",
    "\n",
    "        questions = x['QA_with_answers']\n",
    "        inputs = tokenizer(questions, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to('cuda')\n",
    "        input_length = inputs.attention_mask.sum(dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            original_output = original_model(**inputs)\n",
    "\n",
    "        vectors_patch_loss_list = []\n",
    "        for layer in range(32):\n",
    "            for recover_layer in range(32): # select which layer's MLP value vectors to be recover:\n",
    "                if recover_layer >= layer -2 and recover_layer <= layer + 2:\n",
    "                    model.state_dict()[f'model.layers.{recover_layer}.mlp.down_proj.weight'][:, :] = original_model.state_dict()[f'model.layers.{recover_layer}.mlp.down_proj.weight'][:, :]\n",
    "            \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(**inputs)\n",
    "\n",
    "            loss_list = []\n",
    "            for i, length in enumerate(x['answers_tokens_num']):\n",
    "                loss = compute_loss(original_output.logits[i, -length:, :], output.logits[i, -length:, :])\n",
    "                loss_list.append(loss)\n",
    "\n",
    "            vectors_patch_loss_list.append(loss_list)\n",
    "\n",
    "            model.load_state_dict(old_params)\n",
    "            \n",
    "        vectors_patch_loss_lists.append(vectors_patch_loss_list)   \n",
    "\n",
    "        del model\n",
    "        del tokenizer\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    methods_vectors_patch_loss_lists.append(vectors_patch_loss_lists)\n",
    "    \n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "print('recover on LLaMA by patching on value vectors (5 layer per patch)')\n",
    "        \n",
    "vectors_KRS_list_all = []\n",
    "for t, method in enumerate(['grad_diff', 'dpo']):\n",
    "    print(f'on {method}')\n",
    "    vectors_KRS_list_per_method = []\n",
    "    for patch_losses, ori_losses in zip(methods_vectors_patch_loss_lists[t], methods_ori_loss_list[t]):\n",
    "        vectors_KRS_list_per_data = []\n",
    "        for new_losses in patch_losses: #different_layer\n",
    "            vectors_KRS_list_per_layer = []\n",
    "            for new_loss, ori_loss in zip(new_losses, ori_losses):\n",
    "                KRS = compute_KRS(ori_loss, new_loss=new_loss)\n",
    "                vectors_KRS_list_per_layer.append(KRS.cpu())\n",
    "    \n",
    "            vectors_KRS_list_per_data.append(np.mean(vectors_KRS_list_per_layer, axis=-1))\n",
    "        vectors_KRS_list_per_method.append(vectors_KRS_list_per_data)\n",
    "     \n",
    "    vectors_KRS_list_per_method = np.array(vectors_KRS_list_per_method)\n",
    "    avg_vectors_KRS_list_per_method = np.mean(vectors_KRS_list_per_method, axis=0)\n",
    "    \n",
    "    vectors_KRS_list_all.append(avg_vectors_KRS_list_per_method)\n",
    "\n",
    "vectors_KRS_list_all = np.array(vectors_KRS_list_all)\n",
    "avg_vectors_KRS_list_all = np.mean(vectors_KRS_list_all, axis=0)\n",
    "print(f'avg_vectors_KRS_list_all: {list(avg_vectors_KRS_list_all)}')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
